# Resources

## General

- Linear algebra:
  - [MIT OCW Course â€” Excellent lecture notes!](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/)

## Lecture 2

### Gradient descent
- Why simultaneous update?
  - ["Why should we update simultaneously all the variables in gradient descent?" (Stack Exchange)](https://math.stackexchange.com/questions/2419301/why-should-we-update-simultaneously-all-the-variables-in-gradient-descent/2419310)

- Mini-batch stochastic gradient descent
  - "*A compromise between computing the true gradient and the gradient at a single example is to compute the gradient against more than one training example (called a "mini-batch") at each step.*" [Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)

### Linear regression

- Linear regression to fit nonlinear functions:
  - ["Can a linear regression be quadratic?" (Stack Exchange)](https://math.stackexchange.com/questions/2022783/can-a-linear-regression-be-quadratic)

- Least squares review from linear algebra:
  - [Lecture notes (MIT OCW)](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projection-matrices-and-least-squares/MIT18_06SCF11_Ses2.3sum.pdf)
  - [Videos and materials (MIT OCW)]( https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projection-matrices-and-least-squares/)

- [Why use gradient descent for linear regression, when a closed-form math solution is available? (Stack Exchange)](https://stats.stackexchange.com/questions/278755/why-use-gradient-descent-for-linear-regression-when-a-closed-form-math-solution)

- Convex optimization (proof of convergence of linear regression):
  - Ng brushes over this but seems pretty important. Looking at syllabus, might be covered later?
  - [Slides on convex optimization (Berkeley)](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/optimization/slides.pdf)
  - [Additional slides (NYU; not as clear)](https://cs.nyu.edu/~mohri/mls/ml_convex_optimization.pdf)
  - [Handout (CMU; have not fulled reviewed yet)](http://www.math.cmu.edu/~lohp/docs/math/mop2013/convexity-soln.pdf)

## Lecture 3

### Log Probability
- Why log probability?
  - [Explanation of log probability (Wikipedia)](https://en.wikipedia.org/wiki/Log_probability)

- Review of log properties:
  - [Derivations of log properties (Khan Academy)]( https://www.khanacademy.org/math/algebra2/exponential-and-logarithmic-functions/properties-of-logarithms/a/justifying-the-logarithm-properties)

## Lecture 4

### Newton's Method
- ["Why is Newton's method not widely used in machine learning?" (Stack Exchange)](
https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning)

### Softmax regression
- Derivation of gradient, relation to logistic regression: http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression
http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/
