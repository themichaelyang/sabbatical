## Lecture 2

### Gradient descent
- Why simultaneous update?
  - ["Why should we update simultaneously all the variables in gradient descent?" (Stack Exchange)](https://math.stackexchange.com/questions/2419301/why-should-we-update-simultaneously-all-the-variables-in-gradient-descent/2419310)

### Linear regression

- Linear regression to fit nonlinear functions:
  - ["Can a linear regression be quadratic?" (Stack Exchange)](https://math.stackexchange.com/questions/2022783/can-a-linear-regression-be-quadratic)

- Least squares review from linear algebra:
  - [MIT OCW handout](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projection-matrices-and-least-squares/MIT18_06SCF11_Ses2.3sum.pdf)
  - [MIT OCW videos and materials]( https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projection-matrices-and-least-squares/)

- Convex optimization (proof of convergence of linear regression):
  - Ng brushes over this but seems pretty important. Looking at syllabus, might be covered later?
  - [Slides on convex optimization (Berkeley)](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/optimization/slides.pdf)
  - [Additional slides (NYU; not as clear)](https://cs.nyu.edu/~mohri/mls/ml_convex_optimization.pdf)

## Lecture 3

### Log Probability
- Why log probability?
  - [Explanation of log probability (Wikipedia)](https://en.wikipedia.org/wiki/Log_probability)

- Review of log properties:
  - [Derivations (Khan Academy)]( https://www.khanacademy.org/math/algebra2/exponential-and-logarithmic-functions/properties-of-logarithms/a/justifying-the-logarithm-properties)
